{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow README Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow pandas numpy matplotlib scikit-learn requests beautifulsoup4 tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection from GitHub\n",
    "\n",
    "We'll create a `GitHubDataCollector` class to handle API requests and collect README files from popular repositories in our target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class GitHubDataCollector:\n",
    "    def __init__(self, token: str = None):\n",
    "        self.headers = {}\n",
    "        if token:\n",
    "            self.headers['Authorization'] = f'token {token}'\n",
    "        self.base_url = 'https://api.github.com'\n",
    "        self.rate_limit_remaining = 5000  # Default GitHub API limit\n",
    "        self.rate_limit_reset = 0\n",
    "    \n",
    "    def _check_rate_limit(self):\n",
    "        \"\"\"Check and handle API rate limits\"\"\"\n",
    "        if self.rate_limit_remaining <= 1:\n",
    "            wait_time = max(0, self.rate_limit_reset - time.time())\n",
    "            if wait_time > 0:\n",
    "                print(f'Rate limit reached. Waiting {wait_time:.0f} seconds...')\n",
    "                time.sleep(wait_time)\n",
    "    \n",
    "    def _update_rate_limit(self, response: requests.Response):\n",
    "        \"\"\"Update rate limit info from response headers\"\"\"\n",
    "        self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "        self.rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "    \n",
    "    def get_popular_repos(self, languages: List[str], stars: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get popular repositories for given languages with error handling\"\"\"\n",
    "        repos = []\n",
    "        for lang in tqdm(languages, desc='Fetching repositories'):\n",
    "            try:\n",
    "                self._check_rate_limit()\n",
    "                query = f'language:{lang} stars:>{stars}'\n",
    "                url = f'{self.base_url}/search/repositories?q={query}&sort=stars&per_page=100'\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                self._update_rate_limit(response)\n",
    "                \n",
    "                data = response.json()\n",
    "                repos.extend(data.get('items', []))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'Error fetching {lang} repositories: {e}')\n",
    "        return repos\n",
    "    \n",
    "    def get_readme(self, owner: str, repo: str) -> str:\n",
    "        \"\"\"Get README content with error handling\"\"\"\n",
    "        try:\n",
    "            self._check_rate_limit()\n",
    "            url = f'{self.base_url}/repos/{owner}/{repo}/readme'\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            self._update_rate_limit(response)\n",
    "            \n",
    "            content = response.json()['content']\n",
    "            return base64.b64decode(content).decode('utf-8')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Error fetching README for {owner}/{repo}: {e}')\n",
    "        except (KeyError, UnicodeDecodeError) as e:\n",
    "            print(f'Error processing README for {owner}/{repo}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = GitHubDataCollector()  # Add token='your_token' for authenticated requests\n",
    "\n",
    "# Define target languages and minimum stars\n",
    "languages = ['python', 'javascript', 'java', 'go', 'rust']\n",
    "min_stars = 5000\n",
    "\n",
    "# Collect repositories\n",
    "print(f'Collecting repositories with at least {min_stars} stars...')\n",
    "repos = collector.get_popular_repos(languages, stars=min_stars)\n",
    "print(f'Found {len(repos)} repositories')\n",
    "\n",
    "# Collect READMEs and create dataset\n",
    "data = []\n",
    "for repo in tqdm(repos, desc='Collecting READMEs'):\n",
    "    try:\n",
    "        readme = collector.get_readme(repo['owner']['login'], repo['name'])\n",
    "        if readme:\n",
    "            data.append({\n",
    "                'name': repo['name'],\n",
    "                'language': repo['language'],\n",
    "                'stars': repo['stargazers_count'],\n",
    "                'text': readme\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {repo[\"name\"]}: {e}')\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display dataset statistics\n",
    "print('\\nDataset Statistics:')\n",
    "print(f'Total samples: {len(df)}')\n",
    "print('\\nSamples per language:')\n",
    "print(df['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocess_readme(text: str) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    # Remove code blocks\n",
    "    text = re.sub(r'```[^`]*```', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    \n",
    "    # Remove markdown links\n",
    "    text = re.sub(r'\\[([^\\[]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "# Preprocess all README texts\n",
    "print('Preprocessing README texts...')\n",
    "df['processed_text'] = df['text'].apply(preprocess_readme)\n",
    "\n",
    "# Display example of preprocessed text\n",
    "print('Example of preprocessed text:')\n",
    "print('Original:', df['text'].iloc[0][:200], '...')\n",
    "print('Preprocessed:', df['processed_text'].iloc[0][:200], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text vectorization layer\n",
    "max_features = 10000  # Maximum number of words to keep\n",
    "sequence_length = 500  # Length of each sequence\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the layer to the text data\n",
    "print('Adapting vectorization layer to the text data...')\n",
    "vectorize_layer.adapt(df['processed_text'].values)\n",
    "\n",
    "# Create training and validation sets\n",
    "X = df['processed_text'].values\n",
    "y = pd.get_dummies(df['language']).values  # One-hot encode the labels\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(max_features + 1, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(languages), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "print('Model Architecture Explanation:')\n",
    "print('1. TextVectorization: Converts text to sequences of integer tokens')\n",
    "print('2. Embedding: Converts tokens to dense vectors of size 64')\n",
    "print('3. Bidirectional LSTM: Processes sequences in both directions')\n",
    "print('4. Dense + Dropout: Final classification layers with regularization')\n",
    "print('5. Output: Probability distribution over programming languages')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=6,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print('Training the model...')\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_classes, y_pred_classes, target_names=languages))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=languages, yticklabels=languages)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print('Per-class Accuracy:')\n",
    "for lang, acc in zip(languages, class_accuracy):\n",
    "    print(f'{lang}: {acc:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(text):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_readme(text)\n",
    "    \n",
    "    # Convert to tf.data.Dataset\n",
    "    input_ds = tf.data.Dataset.from_tensor_slices([processed_text]).batch(1)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred = model.predict(input_ds)[0]\n",
    "    \n",
    "    # Return probabilities for each language\n",
    "    return {lang: float(prob) for lang, prob in zip(languages, pred)}\n",
    "\n",
    "# Test with some popular repositories\n",
    "test_repos = [\n",
    "    ('tensorflow/tensorflow', 'Python'),\n",
    "    ('vuejs/vue', 'JavaScript'),\n",
    "    ('golang/go', 'Go'),\n",
    "    ('rust-lang/rust', 'Rust'),\n",
    "    ('spring-projects/spring-boot', 'Java')\n",
    "]\n",
    "\n",
    "for repo, expected_lang in test_repos:\n",
    "    owner, name = repo.split('/')\n",
    "    readme = collector.get_readme(owner, name)\n",
    "    if readme:\n",
    "        predictions = predict_language(readme)\n",
    "        print(f'Repository: {repo}')\n",
    "        print(f'Expected: {expected_lang}')\n",
    "        print('Predictions:')\n",
    "        for lang, prob in sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "            print(f'{lang}: {prob:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model\n",
    "model.save('readme_classifier_model.keras')\n",
    "\n",
    "# Save the vectorization configuration\n",
    "import pickle\n",
    "with open('vectorizer_config.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'config': vectorize_layer.get_config(),\n",
    "        'weights': vectorize_layer.get_weights()\n",
    "    }, f)\n",
    "\n",
    "print('Model and vectorizer saved successfully!')\n",
    "\n",
    "# Example of loading and using the saved model\n",
    "print('Loading and testing saved model...')\n",
    "loaded_model = tf.keras.models.load_model('readme_classifier_model.keras')\n",
    "\n",
    "# Test with a simple Python code snippet\n",
    "sample_text = 'import tensorflow as tf'\n",
    "predictions = predict_language(sample_text)\n",
    "print('Test prediction with loaded model:')\n",
    "for lang, prob in sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "    print(f'{lang}: {prob:.2%}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
